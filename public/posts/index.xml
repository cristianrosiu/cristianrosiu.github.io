<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>All Posts - </title>
        <link>http://example.org/posts/</link>
        <description>All Posts | </description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 19 Jun 2025 21:29:01 &#43;0800</lastBuildDate><atom:link href="http://example.org/posts/" rel="self" type="application/rss+xml" /><item>
    <title>Introduction to Depth Of Field - Part I</title>
    <link>http://example.org/posts/dof-blog/</link>
    <pubDate>Wed, 18 Jun 2025 21:29:01 &#43;0800</pubDate>
    <author>Cristian</author>
    <guid>http://example.org/posts/dof-blog/</guid>
    <description><![CDATA[<p>Depth of Field (DOF) is a popular effect, used to convey a sense of of depth to a large scene, or in some cases, used as an artistic tool to transmit certain emotions and to alter the composition of a scene.</p>
<h2 id="anatomy-of-depth-of-field">Anatomy of Depth of Field</h2>
<p>As the term &ldquo;depth of field&rdquo; intuitively suggests, it describes the range of distances from the camera within which objects appear sharp and fully in focus (Figure 1). Areas positioned closer or farther away than this range become progressively blurred. Before diving deeper into the computational methods and graphics implementations, let&rsquo;s take a moment to explore how DOF occurs naturally in physical cameras .</p>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/bee-dof-camera.png" title="Bee DOF Camera" data-thumbnail="/posts/dof-blog/images/bee-dof-camera.png" data-sub-html="<h2>Figure 1: Simplified diagram of what DOF is. In this case, the DOF is the middle green region where the bee and the flower are in-focus.</h2><p>Bee DOF Camera</p>">
        
    </a><figcaption class="image-caption">Figure 1: Simplified diagram of what DOF is. In this case, the DOF is the middle green region where the bee and the flower are in-focus.</figcaption>
    </figure></p>
<h3 id="real-world-cameras-and-dof">Real-world Cameras and DOF</h3>
<p>To understand DOF intuitively, we should briefly examine the two key physical components of real cameras: the <strong>image sensor</strong> and the <strong>lens</strong>.</p>
<p>The <strong>image sensor</strong> is essentially a flat, two-dimensional grid composed of millions of tiny, light-sensitive pixels. Each pixel collects incoming photons (particles of light), converting them into digital signals that computers or other digital devices process and store as images<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>The <strong>lens</strong> is equally crucial. Its main role is to bend (or refract) incoming light rays, much like the human eye does. Imagine the lens as a carefully shaped piece of glass designed to redirect parallel incoming rays of light to converge precisely onto a single point on the image sensor. Without a lens, these rays would scatter randomly across the sensor, resulting in blurry and indistinct images. Different types of lenses can converge or diverge rays differently, creating diverse visual effects, but the core principle remains consistent.</p>
<p>In real cameras, the DOF range primarily depends on three key parameters<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>:</p>
<ul>
<li><strong>Aperture</strong>: The diameter and shape of the camera&rsquo;s opening.</li>
<li><strong>Focal Length</strong>: The distance from the lens to the image sensor (often called the &ldquo;film-back&rdquo;).</li>
<li><strong>Focal Distance</strong>: Distance from the center of the lens to the real-life desired subject.</li>
</ul>
<h3 id="aperture-and-its-impact-on-dof">Aperture and its Impact on DOF</h3>
<p>The <strong>aperture</strong> refers to the size of the camera&rsquo;s opening through which light enters. In physical terms, the aperture is controlled by an adjustable mechanism known as the <strong>diaphragm</strong>, consisting of multiple overlapping blades arranged in a circular shape. By opening or closing these blades, photographers control how much light enters the camera.</p>
<p>Aperture is measured in <strong>F-stops</strong>, which have an inverse relationship to the diameter of the camera’s opening (Figure 2):</p>
<ul>
<li><strong>Lower F-stop</strong> values correspond to larger apertures (wider openings), allowing more light to enter. Larger apertures result in shallower DOF, creating more pronounced blur effects in the foreground and background.</li>
<li><strong>Higher F-stop</strong> values correspond to smaller apertures (narrower openings), letting in less light but producing sharper images over a larger range of depths, thus less blur overall.</li>
</ul>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/aperture-animation.gif" title="Aperture Animation" data-thumbnail="/posts/dof-blog/images/aperture-animation.gif" data-sub-html="<h2>Figure 2: Simplified graphical animation of how the physical size of the aperture is related to f-stops. This shows the inverse relationship between f-stops and size.</h2><p>Aperture Animation</p>">
        
    </a><figcaption class="image-caption">Figure 2: Simplified graphical animation of how the physical size of the aperture is related to f-stops. This shows the inverse relationship between f-stops and size.</figcaption>
    </figure></p>
<p>Reducing the aperture diameter (increasing F-stop) narrows the cone-shaped paths of the incoming rays. This narrower cone makes it easier for rays originating from different distances to converge closely enough to form a sharper image. At the extreme limit, if the aperture was infinitely small (like a pinhole camera—a theoretical camera model where the aperture is reduced to a tiny pinhole, allowing rays from nearly every distance to converge sharply<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>), you would achieve almost infinite DOF. However, smaller apertures admit less light, potentially causing underexposed (dark) images. Correcting this underexposure through longer exposure times can lead to unwanted motion blur or grainy noise artifacts.</p>
<p>Conversely, wider apertures result in broader cones of incoming rays, increasing divergence and blurring out-of-focus points into a circular shape on the image sensor. This blurred circle has a formal name in photography and rendering: the <strong>Circle of Confusion (CoC)</strong>. All of these concepts will be touched upon in the chapters that will follow.</p>
<h3 id="focal-length-and-zooming">Focal Length and Zooming</h3>
<p>The <strong>focal length</strong> describes the distance from the center of the lens to the image sensor (film-back), typically measured in millimeters (usually ranging from 50mm to 100mm). Adjusting the focal length essentially moves the sensor closer to or further away from the lens, changing how &ldquo;zoomed-in&rdquo; or &ldquo;zoomed-out&rdquo; the resulting image appears. Increasing the focal length (zooming in) narrows your field of view, emphasizing the blur effect in the out-of-focus regions (Figure 3).</p>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/focal-length-animation.gif" title="Focal Length Animation" data-thumbnail="/posts/dof-blog/images/focal-length-animation.gif" data-sub-html="<h2>Figure 3: Visualizing the relationship between lens distances and depth of field. Here, D is the lens-to-object distance, I is the lens-to-filmback distance (also known as focal length), and F represents the lens-to-focal-point distance. Observe that adjusting the lens-to-filmback distance (I) effectively scales the projected image—akin to zooming in and out. Importantly, as you zoom in (increase I), the blur intensifies due to a narrower depth of field; conversely, zooming out (reducing I) sharpens the image, expanding the depth of field.</h2><p>Focal Length Animation</p>">
        
    </a><figcaption class="image-caption">Figure 3: Visualizing the relationship between lens distances and depth of field. Here, D is the lens-to-object distance, I is the lens-to-filmback distance (also known as focal length), and F represents the lens-to-focal-point distance. Observe that adjusting the lens-to-filmback distance (I) effectively scales the projected image—akin to zooming in and out. Importantly, as you zoom in (increase I), the blur intensifies due to a narrower depth of field; conversely, zooming out (reducing I) sharpens the image, expanding the depth of field.</figcaption>
    </figure></p>
<h3 id="focal-distance-or-focal-plane">Focal Distance (or Focal Plane)</h3>
<p>The <strong>focal distance</strong> is simply the distance from the center of the camera lens to the subject of the shot. Changing it will alter what objects are considered in-focus and which are out-of-focus. Think of it as moving an imaginary plane on the camera&rsquo;s forward direction. Any object that is on or close to this imaginary focal plane, will be in-focus. Anything outside the DOF range will be blurred (Figure 4, 5).</p>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/focal-distance-animation.gif" title="Focal Distance Animation" data-thumbnail="/posts/dof-blog/images/focal-distance-animation.gif" data-sub-html="<h2>Figure 4: As the object moves away from the static focal plane, its projection on the image sensor grows.</h2><p>Focal Distance Animation</p>">
        
    </a><figcaption class="image-caption">Figure 4: As the object moves away from the static focal plane, its projection on the image sensor grows.</figcaption>
    </figure></p>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/focal-distance-animation-2.gif" title="Focal Distance Animation 2" data-thumbnail="/posts/dof-blog/images/focal-distance-animation-2.gif" data-sub-html="<h2>Figure 5: The animation shows how the object remains stationary while the focal plane moves. As the plane shifts away, the object drifts out of focus, becoming increasingly blurred.</h2><p>Focal Distance Animation 2</p>">
        
    </a><figcaption class="image-caption">Figure 5: The animation shows how the object remains stationary while the focal plane moves. As the plane shifts away, the object drifts out of focus, becoming increasingly blurred.</figcaption>
    </figure></p>
<h3 id="circle-of-confusion">Circle of Confusion</h3>
<p>When out-of-focus objects project onto the sensor, their light rays do not neatly converge to a single point. Instead, these rays spread evenly across an area, creating a uniformly illuminated circular spot known as the <strong>Circle of Confusion</strong> (Figure 6). The shape of this circle depends on the diaphragm blades: more expensive cameras, having more blades, produce smooth, nearly perfect circles; cheaper cameras with fewer blades produce polygons, often pentagonal or hexagonal shapes. This phenomena is known as &ldquo;<strong>bokeh</strong>&rdquo; (Japanese word for blur) and it refers to the distinctive geometric shapes with which are high in local contrast that are mostly visible in the out-of-focus regions of the image (Figure 7).</p>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/coc-diagram.png" title="CoC Diagram" data-thumbnail="/posts/dof-blog/images/coc-diagram.png" data-sub-html="<h2>Figure 6: The figure describes how light rays form bokeh through a camera lens. Rays from out-of-focus objects don’t converge to a single point—instead, they spread out, forming shapes on the image plane. These shapes, known as bokeh, are defined by the aperture’s geometry: the number and position of the diaphragm blades. On the left, a lens with more blades produces a smoother, more circular bokeh. On the right, fewer blades result in a more polygonal shape, revealing the mechanical structure behind the blur. Figure courtesy of xeolabs, OpenGL Insights.</h2><p>CoC Diagram</p>">
        
    </a><figcaption class="image-caption">Figure 6: The figure describes how light rays form bokeh through a camera lens. Rays from out-of-focus objects don’t converge to a single point—instead, they spread out, forming shapes on the image plane. These shapes, known as bokeh, are defined by the aperture’s geometry: the number and position of the diaphragm blades. On the left, a lens with more blades produces a smoother, more circular bokeh. On the right, fewer blades result in a more polygonal shape, revealing the mechanical structure behind the blur. Figure courtesy of xeolabs, OpenGL Insights.</figcaption>
    </figure></p>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/coc-examples.png" title="CoC Examples" data-thumbnail="/posts/dof-blog/images/coc-examples.png" data-sub-html="<h2>Figure 7: Rendering of different styles of Bokeh shapes. Figure courtesy of xeolabs, OpenGL Insights.</h2><p>CoC Examples</p>">
        
    </a><figcaption class="image-caption">Figure 7: Rendering of different styles of Bokeh shapes. Figure courtesy of xeolabs, OpenGL Insights.</figcaption>
    </figure></p>
<p>The diameter of the CoC (measured in pixels) provides a practical measure of blur intensity, and hence, the DOF is formally defined as the range within which the CoC remains acceptably small<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>:</p>
<ul>
<li>CoC diameter <strong>smaller than one pixel</strong>: points appear sharp and in-focus.</li>
<li>CoC diameter <strong>greater than one pixel</strong>: points blur increasingly as diameter expands.</li>
</ul>
<h3 id="from-real-cameras-to-computer-graphics">From Real Cameras to Computer Graphics</h3>
<p>In computer graphics, the virtual cameras we typically use are based on a simplified theoretical ideal camera model—the <strong>pinhole camera</strong>. As mentioned earlier, a pinhole camera has an infinitesimally small aperture (just a point), allowing rays from any distance to converge sharply onto the sensor, resulting in theoretically perfect focus across infinite depth (Figure 8). This model is just a first order approximation of the mapping from 3D to 2D scenes, because it does not take into consideration lense properties such as geometric distortions or blur.</p>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/pinhole-camera.png" title="Pinhole Camera" data-thumbnail="/posts/dof-blog/images/pinhole-camera.png" data-sub-html="<h2>Figure 8: Comparison of camera models: Above, a pinhole camera allows rays from an object (here, a tree) to pass through a single infinitesimal aperture, creating a perfectly sharp projection without lens-induced distortion or blur. Below, a real camera incorporates a lens, refracting rays so that they converge at a precise focal point before projecting onto the filmback. This lens-based approach accurately simulates realistic optical effects like DOF, enabling scenes to capture nuanced focal transitions absent from the pinhole model.</h2><p>Pinhole Camera</p>">
        
    </a><figcaption class="image-caption">Figure 8: Comparison of camera models: Above, a pinhole camera allows rays from an object (here, a tree) to pass through a single infinitesimal aperture, creating a perfectly sharp projection without lens-induced distortion or blur. Below, a real camera incorporates a lens, refracting rays so that they converge at a precise focal point before projecting onto the filmback. This lens-based approach accurately simulates realistic optical effects like DOF, enabling scenes to capture nuanced focal transitions absent from the pinhole model.</figcaption>
    </figure></p>
<p>However, since real cameras don&rsquo;t behave like perfect pinhole cameras, we need computational methods to approximate real-world blur effects. Besides the accumulation buffer method and raytracing, most techniques do this through the approximation of the CoC value. Two common approaches exist:</p>
<ul>
<li><strong>Physically Accurate CoC</strong>: Derived directly from the camera parameters through equations such as the Thin Lens Equation<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> (Figure 9).</li>
</ul>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/accurate-coc.png" title="Accurate CoC" data-thumbnail="/posts/dof-blog/images/accurate-coc.png" data-sub-html="<h2>Figure 9: The diagram shows how the CoC can be calculated using the physical properties of a real camera. The final formula is derived from the thin lense equation.</h2><p>Accurate CoC</p>">
        
    </a><figcaption class="image-caption">Figure 9: The diagram shows how the CoC can be calculated using the physical properties of a real camera. The final formula is derived from the thin lense equation.</figcaption>
    </figure></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="kt">float</span> <span class="n">coc</span> <span class="o">=</span> <span class="n">abs</span><span class="p">(</span><span class="n">aperture</span> <span class="o">*</span> <span class="p">(</span><span class="n">focallength</span> <span class="o">*</span> <span class="p">(</span><span class="n">objectdistance</span> <span class="o">-</span> <span class="n">planeinfocus</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">objectdistance</span> <span class="o">*</span> <span class="p">(</span><span class="n">planeinfocus</span> <span class="o">-</span> <span class="n">focallength</span><span class="p">)))</span>
</span></span></code></pre></div><ul>
<li><strong>Linear Approximation</strong>: CoC value approximated through a linear function and clamped using Z-buffer values between user-defined bounds<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> (Figure 10).</li>
</ul>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/approximated-coc.png" title="Approximated CoC" data-thumbnail="/posts/dof-blog/images/approximated-coc.png" data-sub-html="<h2>Figure 10: Naturally, the size of the CoC (red line) does not change linearly with the distance of objects from the in-focus regions. A simple linear approximation (blue line) can be used to simplify the parameters and reduce the computational overhead. Figure courtesy of xeolabs, OpenGL Insights.</h2><p>Approximated CoC</p>">
        
    </a><figcaption class="image-caption">Figure 10: Naturally, the size of the CoC (red line) does not change linearly with the distance of objects from the in-focus regions. A simple linear approximation (blue line) can be used to simplify the parameters and reduce the computational overhead. Figure courtesy of xeolabs, OpenGL Insights.</figcaption>
    </figure></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">float4</span> <span class="nf">PSMain</span><span class="p">(</span><span class="n">PS_INPUT</span> <span class="n">input</span><span class="p">)</span><span class="o">:</span> <span class="n">SV_Target</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">float</span> <span class="n">depthNDC</span> <span class="o">=</span> <span class="n">depthBufferTexture</span><span class="p">.</span><span class="n">Sample</span><span class="p">(</span><span class="n">pointClampSampler</span><span class="p">,</span> <span class="n">input</span><span class="p">.</span><span class="n">texCoord</span><span class="p">).</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="kt">float</span> <span class="n">depth</span> <span class="o">=</span> <span class="n">DepthNDCToView</span><span class="p">(</span><span class="n">depthNDC</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl">	<span class="kt">float</span> <span class="n">nearCOC</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">depth</span> <span class="o">&lt;</span> <span class="n">nearEnd</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">nearCOC</span> <span class="o">=</span> <span class="p">(</span><span class="n">depth</span> <span class="o">-</span> <span class="n">nearEnd</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">nearBegin</span> <span class="o">-</span> <span class="n">nearEnd</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">	<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">depth</span> <span class="o">&lt;</span> <span class="n">nearBegin</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">nearCOC</span> <span class="o">=</span> <span class="mf">1.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="n">nearCOC</span> <span class="o">=</span> <span class="n">saturate</span><span class="p">(</span><span class="n">nearCOC</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl">	<span class="kt">float</span> <span class="n">farCOC</span> <span class="o">=</span> <span class="mf">1.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">depth</span> <span class="o">&lt;</span> <span class="n">farBegin</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">farCOC</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">depth</span> <span class="o">&lt;</span> <span class="n">farEnd</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">farCOC</span> <span class="o">=</span> <span class="p">(</span><span class="n">depth</span> <span class="o">-</span> <span class="n">farBegin</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">farEnd</span> <span class="o">-</span> <span class="n">farBegin</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">	<span class="n">farCOC</span> <span class="o">=</span> <span class="n">saturate</span><span class="p">(</span><span class="n">farCOC</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="n">float4</span><span class="p">(</span><span class="n">nearCOC</span><span class="p">,</span> <span class="n">farCOC</span><span class="p">,</span> <span class="mf">0.0f</span><span class="p">,</span> <span class="mf">0.0f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>In practice, CoC values are represented in a range (-1, 1), signifying their relative position to the focal plane:</p>
<ul>
<li>Negative values correspond to the Near Field (closer to camera).</li>
<li>Values around zero represent the Focus Field.</li>
<li>Positive values correspond to the Far Field (further from camera).</li>
</ul>
<h2 id="methods-for-rendering-dof">Methods for Rendering DOF</h2>
<p>To efficiently render DOF effects, graphics pipelines generally separate the image into layers based on their CoC:</p>
<ul>
<li><strong>Focus Field</strong>: Near the focal plane (sharp, CoC &lt; 1px).</li>
<li><strong>Near Field</strong>: Closer than the focal plane (blurred foreground).</li>
<li><strong>Far Field</strong>: Farther away than the focal plane (blurred background).</li>
</ul>
<p>Rendering these layers separately simplifies handling different blending behaviors:</p>
<ul>
<li><strong>Near Field</strong>: Requires blending transparency at edges (semi-transparent blur).</li>
<li><strong>Far Field</strong>: Must avoid haloing around sharp objects, handled carefully.</li>
</ul>
<p>Most methods follow this workflow (also see Figure 11):</p>
<ol>
<li>Calculate per-pixel CoC from depth buffer.</li>
<li>Split image into layers using CoC masks.</li>
<li>Downsample and apply blur to layers independently.</li>
</ol>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/coc-pipeline.png" title="/posts/dof-blog/images/coc-pipeline.png" data-thumbnail="/posts/dof-blog/images/coc-pipeline.png" data-sub-html="<h2>Figure 11: The figure shows the most common pipeline used to compute DOF. The first step is to generate downsampled CoC and color textures. Then, the CoC is used to mask the foreground pixels to the rest. Finally, the blur is computed separately for each masked texture and the results are composited together.</h2>">
        
    </a><figcaption class="image-caption">Figure 11: The figure shows the most common pipeline used to compute DOF. The first step is to generate downsampled CoC and color textures. Then, the CoC is used to mask the foreground pixels to the rest. Finally, the blur is computed separately for each masked texture and the results are composited together.</figcaption>
    </figure></p>
<p>In subsequent chapters, we explore various DOF rendering techniques—including Scatter-Based, Gather-Based, and Scatter-As-You-Gather methods—in greater detail.</p>
<h3 id="simulating-dof-ground-truth">Simulating DOF (Ground Truth)</h3>
<p>To truly understand and evaluate DOF rendering methods in computer graphics, it was important to have a reliable reference—a sort of &ldquo;ground truth&rdquo; to compare against. Such a ground truth could be simulated accurately using an approach known as the <strong>Accumulation Buffer (AB)</strong> method. An AB is just a high precision color buffer that is used to store multiple images inside of it<sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>.</p>
<p>Imagine you&rsquo;re holding a real camera, focusing precisely on an object placed at a certain distance—the focal plane. If you slightly move the camera around this focal plane while always keeping it pointed directly toward it, you&rsquo;ll notice something interesting:</p>
<ul>
<li>Objects exactly on or near the focal plane stay relatively sharp because their position relative to the sensor doesn&rsquo;t change significantly.</li>
<li>Objects far away from this plane (either closer or further) shift more dramatically across your sensor from one camera position to another. As you average these multiple views, these areas naturally blur.</li>
</ul>
<p>In the world of computer graphics, the <strong>Accumulation Buffer method</strong> digitally replicates this intuitive physical process. The focal plane stayed fixed, and the virtual camera&rsquo;s position is moved slightly multiple times, each time capturing a snapshot of the scene from these different viewpoints (Figure 12). These snapshots are then combined—summed up and averaged—to produce the final image.</p>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/dof-simulation.png" title="DOF Simulation" data-thumbnail="/posts/dof-blog/images/dof-simulation.png" data-sub-html="<h2>Figure 12: Image shows the simplified process of accurately simulating DOF. In the accumulation buffer technique, the focal plane of the camera is kept stationary while its positions changes, taking a snapshot at each step. Processing all snapshots results in a accurate representation of DOF. Figure courtesy of Akenine-Möller et al., Real-Time Rendering, 4th Edition.</h2><p>DOF Simulation</p>">
        
    </a><figcaption class="image-caption">Figure 12: Image shows the simplified process of accurately simulating DOF. In the accumulation buffer technique, the focal plane of the camera is kept stationary while its positions changes, taking a snapshot at each step. Processing all snapshots results in a accurate representation of DOF. Figure courtesy of Akenine-Möller et al., Real-Time Rendering, 4th Edition.</figcaption>
    </figure></p>
<p>The more snapshots or samples taken, the closer the resulting image is to real-world accuracy. A limited number of snapshot can result in ghosting, just straight up copying of objects, or bending artifacts when high blur is present. As a rule of thumb, you can estimate the number of accumulation passes required for realistic DOF by dividing the area of the largest CoC by the number of pixels you’re willing to tolerate per sample. For high-quality results with minimal banding (limited to about 2×2 pixel blocks), aim for one pass per 4 pixels of CoC area—for example, a CoC with an 8-pixel radius (≈200 pixels in area) would require about 50 passes (200 /4). For a more performance-friendly but lower-quality approximation, you can stretch that to one pass per 9 pixels of CoC area, tolerating up to 3×3 pixel blocks of blur and reducing the pass count significantly—for instance, down to about 12 passes for a 6-pixel-radius CoC. This trade-off between visual fidelity and rendering cost is key when tuning for real-time vs. offline rendering<sup id="fnref2:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>. Consequently, there is a significant cost: rendering each viewpoint separately means that this technique is computationally expensive. Because of this, the accumulation buffer method is rarely used directly during real-time rendering. Instead, it serves as a reliable benchmark, allowing developers and artists to qualitatively compare and refine faster but less physically accurate DOF approximations.</p>
<p>All subsequent rendering methods we discuss attempted to approximate this accurate simulation in different ways—each exploring and solving the DOF problem from various perspectives, especially in terms of efficiently approximating the CoC.</p>
<h3 id="scatter-based-forward-mapping-method">Scatter-Based (Forward Mapping) Method</h3>
<p>In scatter-based DOF techniques, each pixel scatters its shading value to its neighbours. Unfortunately, scattering operations do not map well to current shader capabilities as it requires some level of synchronisation that would affect the parallel nature of GPUs. One solution that worked well in practice is the usage of sprites. Each out-of-focus pixel is transformed into a <strong>sprite</strong>—a piece of geometry, typically a rectangle or circle, with a texture attached to it —that represents how much that pixel should blur across the screen. The size of this sprite is driven by the <strong>CoC</strong>, a value derived from the z-buffer.</p>
<p>To build an intuition, imagine a <strong>white square the size of 1 pixel</strong> floating in 2D space. If that pixel is in focus, it stays sharp—affecting only its original screen-space location. We now take that tiny white quad and <strong>scale it up</strong> using the CoC value, spreading it in nearby pixels. The result? Its contribution <strong>overlaps neighboring pixels</strong>.</p>
<p>So rather than computing blur by pulling in nearby samples (as in gather-based methods), <strong>each pixel scatters its own influence outward</strong>, like dropping a pebble and watching ripples expand.</p>
<p>In practice, this is often implemented using a <strong>Geometry Shader</strong>, a programmable stage in the GPU pipeline that can dynamically emit geometry based on input data. Here, it reads the pixel’s depth, calculates its CoC, and then <strong>emits a triangle-based sprite</strong>, scaled accordingly<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup> (see Figure 13).</p>
<p>Geometry information of the sprite is then sent to a Pixel Shader (PS), where a custom bokeh texture is sampled, and each sprite rasterised and blended into the final image using <strong>additive alpha blending</strong>. Additive blending combines the colors of overlapping sprites, creating a cumulative blur effect. Finally, these sprites are composited into a single coherent image by averaging, normalizing, and upscaling the resulting blurred layers.</p>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/scatter-dof.png" title="/posts/dof-blog/images/scatter-dof.png" data-thumbnail="/posts/dof-blog/images/scatter-dof.png" data-sub-html="<h2>Figure 13: Stages of the scatter-based method using sprites. The CoC is calculated from the z-buffer values then sent to the geometry shader for the creation of triangle or quad based geometry. Finally, all pixels of each sprite sample a custom aperture texture and result is accumulated and composited in the scene using alpha blending. Figure courtesy of Tiago Sousa, Graphics Gems from CryENGINE 3</h2>">
        
    </a><figcaption class="image-caption">Figure 13: Stages of the scatter-based method using sprites. The CoC is calculated from the z-buffer values then sent to the geometry shader for the creation of triangle or quad based geometry. Finally, all pixels of each sprite sample a custom aperture texture and result is accumulated and composited in the scene using alpha blending. Figure courtesy of Tiago Sousa, Graphics Gems from CryENGINE 3</figcaption>
    </figure></p>
<p>This process is intuitive and straightforward: it is called &ldquo;scattering&rdquo; because each blurred pixel scatters its influence outward onto surrounding pixels. However, the scatter-based method faces significant challenges when executed on GPUs:</p>
<ul>
<li>
<p>Graphics Processing Units (GPUs) are designed around massive parallelisation—running thousands of threads simultaneously to render images rapidly. This parallelism relied on threads operating independently with minimal synchronisation. Scattering is equivalent to a write operation, however, doing so on GPUs require multiple threads to coordinate frequently, as they have to write results to overlapping pixel locations simultaneously, causing synchronisation bottlenecks<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>. This is partially solved through the usage of sprites, however, the variability in performance yielded by such methods makes them less suitable for real-time applications<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>. This variability is mostly given by the high fill rate and bandwidth required by drawing a quad for each pixel<sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>.</p>
</li>
<li>
<p>Scatter-based methods are notoriously tricky when combined with other translucent effects such as smoke, fog, or particle systems. Since each of these effects also involved transparency and blending, compositing them with scatter-based DOF methods became complicated. Often, developers had to give artists explicit control over the order in which different effects or materials were composited<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup> (e.g., DOF before or after particles), adding complexity and additional time to the artistic workflow (Figure 14)</p>
</li>
</ul>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/scatter-dof-issue.png" title="/posts/dof-blog/images/scatter-dof-issue.png" data-thumbnail="/posts/dof-blog/images/scatter-dof-issue.png" data-sub-html="<h2>Figure 14: The figure shows special shader graph nodes created to help artist better compiste the DOF post-process effect with other transparent effects. Figure courtesy of Martin Mittring et. al, The Technology Behind the DirectX 11 Unreal Engine Samaritan Demo.</h2>">
        
    </a><figcaption class="image-caption">Figure 14: The figure shows special shader graph nodes created to help artist better compiste the DOF post-process effect with other transparent effects. Figure courtesy of Martin Mittring et. al, The Technology Behind the DirectX 11 Unreal Engine Samaritan Demo.</figcaption>
    </figure></p>
<h3 id="gather-based-backward-mapping">Gather-Based (Backward Mapping)</h3>
<p>To better understand the gather-based method, let&rsquo;s first revisit briefly what we learned about scatter-based (forward mapping) methods. Recall that in scatter-based methods, each pixel &ldquo;spreads&rdquo; its blur to nearby pixels. While straightforward conceptually, this isn&rsquo;t very GPU-friendly due to synchronization complexities.</p>
<p><strong>Gather-based methods</strong>, on the other hand, flip this logic upside down. Instead of scattering, each pixel &ldquo;gathers&rdquo; or collects information from neighboring pixels around it (Figure 15). Imagine you&rsquo;re trying to figure out the exact shade of color your pixel should be. Instead of telling your neighbors, &ldquo;Here’s my blur!&rdquo;, you ask your neighbors, &ldquo;What blur should I be seeing here?&rdquo; This subtle yet important inversion aligns very naturally with GPU architectures because GPUs excel at sampling data from nearby memory locations. Sampling nearby pixels—essentially reading memory that&rsquo;s close together<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>—is exactly the kind of task GPUs do exceptionally efficiently . This hardware capability makes gather-based approaches attractive from both a performance and implementation standpoint</p>
<div class="details admonition note open">
        <div class="details-summary admonition-title">
            <i class="icon fas fa-pencil-alt fa-fw" aria-hidden="true"></i>Note<i class="details-icon fas fa-angle-right fa-fw" aria-hidden="true"></i>
        </div>
        <div class="details-content">
            <div class="admonition-content"><p>Most of GPUs and Graphics APIs offer the option for hardware-based gather operations on GPU resources (e.g. textures). Check the code for an example written in HLSL.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-hlsl" data-lang="hlsl"><span class="line"><span class="cl"><span class="kt">float4</span> <span class="n">samples</span> <span class="o">=</span> <span class="n">myTexture</span><span class="p">.</span><span class="n">GatherRed</span><span class="p">(</span><span class="n">mySampler</span><span class="p">,</span> <span class="n">uv</span> <span class="o">+</span> <span class="kt">float2</span><span class="p">(</span><span class="mf">0.5f</span><span class="p">,</span> <span class="mf">0.5f</span><span class="p">)</span> <span class="o">/</span> <span class="n">myTextureDim</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="kt">float</span> <span class="n">r1</span> <span class="o">=</span> <span class="n">samples</span><span class="p">.</span><span class="n">w</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="kt">float</span> <span class="n">r2</span> <span class="o">=</span> <span class="n">samples</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="kt">float</span> <span class="n">r3</span> <span class="o">=</span> <span class="n">samples</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="kt">float</span> <span class="n">r4</span> <span class="o">=</span> <span class="n">samples</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
</span></span></code></pre></div></div>
        </div>
    </div>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/scatter-and-gather-op.png" title="Scatter And Gather Operation" data-thumbnail="/posts/dof-blog/images/scatter-and-gather-op.png" data-sub-html="<h2>Figure 15: Difference between the scatter-based (left) and gather-based (right) methods. Figure courtesy of Akenine-Möller et al., Real-Time Rendering, 4th Edition.</h2><p>Scatter And Gather Operation</p>">
        
    </a><figcaption class="image-caption">Figure 15: Difference between the scatter-based (left) and gather-based (right) methods. Figure courtesy of Akenine-Möller et al., Real-Time Rendering, 4th Edition.</figcaption>
    </figure></p>
<p>The unfortunate aspect about these methods is the <strong>Neighborhood Assumption</strong>. Gathering inherently assumes that neighboring pixels have similar depth values, meaning they&rsquo;re part of the same object or closely related objects. However, at object boundaries—where depth values shift drastically—this assumption can break down, leading to visible artifacts like haloing.</p>
<p>Let&rsquo;s visualize this haloing effect intuitively: imagine you&rsquo;re sampling colors for a pixel at the edge of a in-focus foreground object. Some gathered samples accidentally include background pixels due to a large CoC radius. Blending these distant pixels creates unwanted halos around the object&rsquo;s edge, breaking visual realism (Figure 16).</p>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/dof-problem-1.png" title="DOF Problem 1" data-thumbnail="/posts/dof-blog/images/dof-problem-1.png" data-sub-html="<h2>Figure 16: Visualizing common artifacts in gather-based DOF rendering. On the left, the character&#39;s nose is sharply focused against a blurred background. When sampling blurred background pixels near this sharp edge, the gather-based method unintentionally mixes in focused foreground data (due to high CoC radius), causing a noticeable halo around the nose. On the right, the reverse scenario occurs—the character is blurred while the background remains sharp. Here, sharp background details (like the sky) bleed onto the blurred edges of the character. Both cases highlight the fundamental challenge with gather-based methods: they indiscriminately blend pixel information across depth boundaries, resulting in visual artifacts known as haloing and color bleeding. Figure courtesy of Tiago Sousa, Graphics Gems from CryENGINE 3</h2><p>DOF Problem 1</p>">
        
    </a><figcaption class="image-caption">Figure 16: Visualizing common artifacts in gather-based DOF rendering. On the left, the character's nose is sharply focused against a blurred background. When sampling blurred background pixels near this sharp edge, the gather-based method unintentionally mixes in focused foreground data (due to high CoC radius), causing a noticeable halo around the nose. On the right, the reverse scenario occurs—the character is blurred while the background remains sharp. Here, sharp background details (like the sky) bleed onto the blurred edges of the character. Both cases highlight the fundamental challenge with gather-based methods: they indiscriminately blend pixel information across depth boundaries, resulting in visual artifacts known as haloing and color bleeding. Figure courtesy of Tiago Sousa, Graphics Gems from CryENGINE 3</figcaption>
    </figure></p>
<p>Gather-based (backward mapping) methods elegantly leverage GPU strengths by shifting complexity away from scattering information to intelligently gathering it. While highly efficient and broadly effective, handling edge cases at object boundaries remains a nuanced challenge, necessitating careful management of sampling strategies and edge-aware filtering techniques.</p>
<h3 id="scatter-as-you-gather-hybrid-method">Scatter-as-you-Gather (Hybrid Method)</h3>
<p>We&rsquo;ve previously discussed two distinct approaches for rendering DOF: the <strong>Scatter-Based</strong> method, where each pixel actively spreads its color to neighboring pixels, and the <strong>Gather-Based</strong> method, where each pixel passively pulls colors from its neighbors. Each approach comes with its unique advantages and inherent limitations. Interestingly, there&rsquo;s a clever hybrid approach—often called the <strong>Scatter-as-you-Gather</strong> method—that blends the strengths of both these worlds.</p>
<p>To combine the efficiency of gathering with the precision of scattering, we use a hybrid method: <strong>scatter-as-you-gather</strong>. This method smartly flips the scattering problem on its head. Instead of each pixel blindly pushing its color outward, pixels carefully examine their neighbors and selectively decide whether a neighbor’s color should &ldquo;scatter&rdquo; into them<sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup> <sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup>.</p>
<p>Here&rsquo;s an intuitive breakdown of the process (see <strong>Figure 17</strong> and <strong>Figure 18</strong> for a visual overview):</p>
<ol>
<li><strong>Gather Phase</strong>: For each pixel, we first gather color and depth information from neighboring pixels within a radius defined by the current pixel’s own CoC. Remember, the CoC size indicates how blurry a pixel should appear—the larger the CoC, the more blurred the pixel.</li>
<li><strong>Scatter Decision (Depth-Based Selection)</strong>: After collecting these neighboring pixels, the algorithm evaluates each neighbor’s own CoC to determine if and how their blurred color could affect the current pixel. Think of this as each pixel &ldquo;asking&rdquo; its neighbors: <strong>“Does your blur overlap with my location?”</strong>. To resolve conflicts—where multiple neighboring pixels potentially scatter into one pixel—the algorithm prioritizes by depth: the pixel closest to the camera (lowest z-depth) is chosen as the <strong>near image</strong>. Remaining neighbors close in depth to the neighbor with lowest z-depth are combined using alpha-blending, averaged together and stored in a <strong>foreground</strong> texture. Blending pixels in this manner gets rid of the need for sorting based on depth (which is a common performance bottleneck)</li>
<li><strong>Background Layering and Final Composition</strong>: Pixels that do not significantly scatter into our current pixel (usually those further back or less blurry) are grouped into a separate background layer. Finally, the carefully crafted foreground layer is composited over this background layer using alpha blending to produce the final, visually coherent result.</li>
</ol>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/scatter-as-gather-process.png" title="Scatter As Gather Process" data-thumbnail="/posts/dof-blog/images/scatter-as-gather-process.png" data-sub-html="<h2>Figure 17: Illustration of the scatter-as-gather approach for DOF rendering. At the center (yellow square) is the current pixel being processed. Surrounding blue dots represent potential sampling positions, defined by the pixel&#39;s CoC radius. Contributions are accumulated from neighboring pixels (white dots) whose blur areas (colored circles) overlap this central pixel. Importantly, these contributions are gathered in depth order—from closest (red) to furthest (magenta)—ensuring proper occlusion and accurate blending of blurred elements. Figure courtesy of Jorge Jimenez, Next Generation Post Processing in Call of Duty: Advanced Warfare</h2><p>Scatter As Gather Process</p>">
        
    </a><figcaption class="image-caption">Figure 17: Illustration of the scatter-as-gather approach for DOF rendering. At the center (yellow square) is the current pixel being processed. Surrounding blue dots represent potential sampling positions, defined by the pixel's CoC radius. Contributions are accumulated from neighboring pixels (white dots) whose blur areas (colored circles) overlap this central pixel. Importantly, these contributions are gathered in depth order—from closest (red) to furthest (magenta)—ensuring proper occlusion and accurate blending of blurred elements. Figure courtesy of Jorge Jimenez, Next Generation Post Processing in Call of Duty: Advanced Warfare</figcaption>
    </figure></p>
<p><figure><a class="lightgallery" href="/posts/dof-blog/images/scatter-as-gather-rtr.png" title="Scatter As Gather Process" data-thumbnail="/posts/dof-blog/images/scatter-as-gather-rtr.png" data-sub-html="<h2>Figure 18: Illustration of the scatter-as-gather approach for DOF rendering. At the center (square with black dot) is the current pixel being processed. The left image shows the sampled neighbors, while the right image shows their CoC radii. It can be observed that only the orange and red CoCs overlap with the center pixel, and therefore , only their contribution added in the order of their distance to the camera. Figure courtesy of Akenine-Möller et al., Real-Time Rendering, 4th Edition.</h2><p>Scatter As Gather Process</p>">
        
    </a><figcaption class="image-caption">Figure 18: Illustration of the scatter-as-gather approach for DOF rendering. At the center (square with black dot) is the current pixel being processed. The left image shows the sampled neighbors, while the right image shows their CoC radii. It can be observed that only the orange and red CoCs overlap with the center pixel, and therefore , only their contribution added in the order of their distance to the camera. Figure courtesy of Akenine-Möller et al., Real-Time Rendering, 4th Edition.</figcaption>
    </figure></p>
<p>So, to summarize:</p>
<ul>
<li><strong>Reduced Haloing and Edge Artifacts</strong>: By carefully selecting neighboring pixels based on their CoC and depth, this method reduces problematic halos around objects. It&rsquo;s particularly effective in rendering realistic transitions between blurry foreground objects and the sharp focus region behind them.</li>
<li><strong>Efficient GPU Implementation</strong>: Although the method introduces an extra complexity step—evaluating neighbors—it remains highly compatible with GPU architectures. GPUs excel at independent gather operations, and this hybrid approach leverages that strength while mimicking scattering through a carefully controlled selection process.</li>
<li><strong>No Need for Depth Sorting</strong>: Traditional scatter-based methods might require sorting pixels to correctly composite translucent layers—a computationally expensive process. The hybrid method sidesteps this requirement entirely through smart alpha blending, achieving visually accurate results without performance hits.</li>
</ul>
<p>Despite its strengths, this method isn’t without trade-offs. The shader logic can become quite intricate, especially when handling edge cases involving large blur radii or dense depth discontinuities. More blur means more neighbors to evaluate, which can add up computationally. In practice, optimizations or approximations are often needed to maintain performance in high-CoC scenarios.</p>
<h2 id="final-thoughts-and-next-steps">Final Thoughts and Next Steps</h2>
<p>Depth of field is one of those cool effects that combines photography and code in a way that&rsquo;s both approachable and satisfying—especially if you&rsquo;re new to computer graphics. My goal with these posts is to make graphics more accessible, and DOF is a great beginner-friendly starting point. Stay tuned for part two, where I&rsquo;ll dive into implementing one of these methods step-by-step. If you enjoyed this and want to help support more rendering blogs, you can help me with a <a href="https://www.paypal.com/donate/?hosted_button_id=TQA9LV2HACZDN" target="_blank" rel="noopener noreffer ">coffe</a>!</p>
<h2 id="references">References</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://thinklucid.com/tech-briefs/understanding-digital-image-sensors/" target="_blank" rel="noopener noreffer ">LUCID Vision Labs - Understanding The Digital Image Sensor</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://dev.epicgames.com/documentation/en-us/unreal-engine/cinematic-depth-of-field-in-unreal-engine" target="_blank" rel="noopener noreffer ">Cinematic Depth of Field, Unreal Engine</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://ia800704.us.archive.org/32/items/crytek_presentations/Sousa_Graphics_Gems_CryENGINE3.pdf" target="_blank" rel="noopener noreffer ">Graphics Gems from CryENGINE 3, by Tiago Sousa</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Matt Pharr and Greg Humphreys. Physically Based Rendering,
Second Edition: From Theory To Implementation, Second edition. San Francisco, CA:
Morgan Kaufmann Publishers Inc., 2010.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p><a href="https://developer.nvidia.com/gpugems/gpugems3/part-iv-image-effects/chapter-28-practical-post-process-depth-field" target="_blank" rel="noopener noreffer ">GPU Gems 3 - Chapter 28. Practical Post-Process Depth of Field</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p><a href="https://developer.nvidia.com/gpugems/gpugems/part-iv-image-processing/chapter-23-depth-field-survey-techniques" target="_blank" rel="noopener noreffer ">GPU Gems 1 - Chapter 23. Depth of Field: A Survey of Techniques</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref2:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p><a href="https://xeolabs.com/pdfs/OpenGLInsights.pdf" target="_blank" rel="noopener noreffer ">OpenGL Insights - xeolabs</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>GPU Zen: Advanced Rendering Techniques, by Wolfgang Engel&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p><a href="https://bartwronski.com/2014/04/07/bokeh-depth-of-field-going-insane-part-1/" target="_blank" rel="noopener noreffer ">Bokeh depth of field – going insane! part 1, by Bart Wronski</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p><a href="https://developer.nvidia.com/gpugems/gpugems2/part-iv-general-purpose-computation-gpus-primer/chapter-32-taking-plunge-gpu" target="_blank" rel="noopener noreffer ">GPU Gems 2 - Chapter 32. General Purpose Computing GPUS Primer</a>&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Real-Time Rendering, 4th Edition, by Tomas Akenine-Möller&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p><a href="https://www.gdcvault.com/play/1014666/-SPONSORED-The-Technology-Behind" target="_blank" rel="noopener noreffer ">The Technology Behind the DirectX 11 Unreal Engine &ldquo;Samaritan&rdquo; Demo, by Martin Mittring et. al</a>&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>Nocentino, Anthony E., and Philip J. Rhodes. &ldquo;Optimizing memory access on GPUs using morton order indexing.&rdquo; Proceedings of the 48th annual ACM Southeast Conference. 2010.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p><a href="https://www.iryoku.com/next-generation-post-processing-in-call-of-duty-advanced-warfare/" target="_blank" rel="noopener noreffer ">Next Generation Post Processing in Call of Duty: Advanced Warfare, by Jorge Jimenez</a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>]]></description>
</item>
<item>
    <title>Real-time Rendering of Water Reflections in VR</title>
    <link>http://example.org/posts/water-research/</link>
    <pubDate>Thu, 19 Jun 2025 21:29:01 &#43;0800</pubDate>
    <author>Cristian</author>
    <guid>http://example.org/posts/water-research/</guid>
    <description><![CDATA[<p>Rendering accurate water reflections is crucial for achieving realism in computer graphics. Their integration with VR and AR technologies can further elevate experiences in sectors like education, engineering, and medicine.</p>
<h3 id="abstract-paperhttpsrepositorytudelftnlrecorduuid6a14ca81-a277-4a1b-b45f-b6ac50b45045">Abstract (<a href="https://repository.tudelft.nl/record/uuid:6a14ca81-a277-4a1b-b45f-b6ac50b45045" target="_blank" rel="noopener noreffer ">Paper</a>)</h3>
<p>Even though various monoscopic reflection techniques exist they either demand substantial computational resources or cannot be straightforwardly adapted to stereoscopic media, leaving a big hole in research concerning stereo-aware reflections and their applications. This thesis researches and analyses the different techniques for water reflections. Further, it makes significant strides in the development of stereo-consistent real-time water reflections for XR platforms. This is achieved by combining implicit functions and bounding volume hierarchies (BVH) into a unified system called Adaptive Hierarchical Signed Distance Fields Reflections (AH-SDFR). It was found that AH-SDF outperforms leading methods such as Hierarchical Screen Space Reflections (HSSR) and Reflection Probes in both visual quality and performance, and as a result, it offers a promising solution for XR applications. However, challenges persist, mainly due to the large memory demands of intricate 3D scenes and perceptual errors from inaccuracies in volume textures.</p>]]></description>
</item>
<item>
    <title>Mathisfactory: An Augmented Reality Game for Reinforcing Mathematical Intuition in Earth Sciences</title>
    <link>http://example.org/posts/mathisfactory/</link>
    <pubDate>Wed, 18 Jun 2025 21:29:01 &#43;0800</pubDate>
    <author>Cristian</author>
    <guid>http://example.org/posts/mathisfactory/</guid>
    <description><![CDATA[<p>Mathisfactory is an Augmented Reality (AR) serious game designed to enhance students&rsquo; understanding of abstract mathematical concepts such as level curves and directional derivatives.</p>
<h3 id="abstract-paperhttpsgithubcomcristianrosiumathisfactoryblobmainreportpdf">Abstract (<a href="https://github.com/cristianrosiu/mathisfactory/blob/main/report.pdf" target="_blank" rel="noopener noreffer ">Paper</a>)</h3>
<p>By utilizing AR technology, the game provides an interactive and immersive learning experience for students, bridging the gap between theoretical knowledge and real-world application. The game is set in a geological context, with players analyzing and predicting lava flow in a volcanic situation using their understanding of level curves and directional derivatives.</p>]]></description>
</item>
<item>
    <title>Multi-task Seals Auscultation Classification</title>
    <link>http://example.org/posts/auscultations-research/</link>
    <pubDate>Tue, 17 Jun 2025 21:29:01 &#43;0800</pubDate>
    <author>Cristian</author>
    <guid>http://example.org/posts/auscultations-research/</guid>
    <description><![CDATA[<p>Pulmonary auscultation is one of the most valuable and fundamental tools available to veterinarians to assess lung conditions in animals quickly</p>
<h3 id="abstract-paperhttpsgithubcomcristianrosiudeep-auscultation-classifierblobmainpaperpdf">Abstract (<a href="https://github.com/cristianrosiu/deep-auscultation-classifier/blob/main/paper.pdf" target="_blank" rel="noopener noreffer ">Paper</a>)</h3>
<p>Despite all the advances in the medical field, electronic chest auscultation can sometimes represent an unreliable method due to the non-stationary property of lung sounds. Automating this process can aid the clinicians in their diagnosis process and hopefully improve the survival rate of seals that arrive at the Pieterburen Zeehondencentrum in Groningen. One way to do this is through means of deep learning. However, most of the time, audio classification tasks are usually treated as independent tasks. As lung sounds are known to be related to one another in some form or shape, a single-task approach, by focusing only on one task, misses most of the information necessary to make a difference when classifying closely related tasks. This paper aims to show the potential of multi-task learning (MTL) in the context of seals lung sound classification. We proposed three different types of multi-task convolutional neural network architectures. These models are evaluated on the mel-cepstral coefficients (MFCCs) features and per-channel energy normalized spectrograms (PCEN). Experiments are conducted on a dataset of 142 samples gathered from both the left and right lungs of multiple seals. The two types of abnormal sounds present in this dataset are Wheezing and Rhonchus. Results show that the MFCC features, together with our custom-built CNN obtained an accuracy of 73% when classifying wheezing and 63% in the case of rhonchus, outperforming the classification of PCEN images by 15% and 25% respectively. Lastly, the same model manage to obtain a survival prediction accuracy of 80% and succesfully showing the potential of MTL in auscultation classification.</p>]]></description>
</item>
</channel>
</rss>
